{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U pandas==1.5.3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's extract the dataset from its archive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "dataset = \"data/1000000 Sales Records.zip\"\n",
    "# Extracting the dataset from its archive\n",
    "with zipfile.ZipFile(\"data/1000000 Sales Records.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading the Neo4j Spark Connector jar, but only if it's not present"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wget,os\n",
    "\n",
    "# Url of Neo4j Connector for Spark Jar\n",
    "url = 'https://github.com/neo4j-contrib/neo4j-spark-connector/releases/download/4.1.5/neo4j-connector-apache-spark_2.12-4.1.5_for_spark_3.jar'\n",
    "\n",
    "# Getting the name of the Spark Connector from its URL\n",
    "filename = url.split(\"/\")[-1]\n",
    "\n",
    "# Download it only if it's not present\n",
    "if filename not in os.listdir():\n",
    "    print(\"Downlading Spark Connector\")\n",
    "    filename = wget.download(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start the SparkSession by providing also the Neo4j Spark Connector jar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Providing the downloaded jar to the pyspark-shell\n",
    "os.environ[\n",
    "    'PYSPARK_SUBMIT_ARGS'] = f'--jars {filename} pyspark-shell'\n",
    "\n",
    "#Ip address of the Neo4j database, you need also to provide its port number\n",
    "NEO4J_IP_ADDRESS = \"ip_address_of_neo4j:bolt_port_number(usually is 7687)\"\n",
    "\n",
    "# Building our SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1536m\").\\\n",
    "        getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now load the DataFrame of our dataset to our Spark Cluster"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating the Spark dataFrame\n",
    "df_pd = pd.read_csv(dataset)\n",
    "sparkDF=spark.createDataFrame(df_pd)\n",
    "sparkDF.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the nodes with label Country"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the data from our SparkDataframe before writing it to Neo4j\n",
    "countryValuesDF = sparkDF\\\n",
    "                    .select('Country')\\\n",
    "                    .distinct()\\\n",
    "                    .withColumnRenamed(\"Country\", \"countryName\")\n",
    "\n",
    "# Example of creating nodes by using the Connector options\n",
    "countryValuesDF.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", f\"bolt://{NEO4J_IP_ADDRESS}\")\\\n",
    ".option(\"authentication.basic.username\", \"neo4j\")\\\n",
    ".option(\"authentication.basic.password\", \"test\")\\\n",
    ".option(\"batch.size\", \"5000\")\\\n",
    ".option(\"database\", \"neo4j\")\\\n",
    ".option(\"labels\",\"Country\")\\\n",
    ".option(\"node.keys\",\"countryName\")\\\n",
    ".option(\"script\",\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (t:Country) REQUIRE t.countryName IS UNIQUE;\"\"\")\\\n",
    ".save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the nodes with label Region"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the data from our SparkDataframe before writing it to Neo4j\n",
    "regionValuesDF = sparkDF\\\n",
    "                    .select('Region')\\\n",
    "                    .distinct()\\\n",
    "                    .withColumnRenamed(\"Region\", \"regionName\")\n",
    "\n",
    "print(f\"Ingesting {regionValuesDF.count()} nodes of type Country\")\n",
    "\n",
    "# Example of creating nodes by using the Connector options\n",
    "regionValuesDF.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", f\"bolt://{NEO4J_IP_ADDRESS}\")\\\n",
    ".option(\"authentication.basic.username\", \"neo4j\")\\\n",
    ".option(\"authentication.basic.password\", \"test\")\\\n",
    ".option(\"batch.size\", \"5000\")\\\n",
    ".option(\"database\", \"neo4j\")\\\n",
    ".option(\"labels\",\"Region\")\\\n",
    ".option(\"node.keys\",\"regionName\")\\\n",
    ".option(\"script\",\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (t:Region) REQUIRE t.regionName IS UNIQUE;\"\"\")\\\n",
    ".save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the relationship that connects nodes of type Country with Nodes of type Region (IN_REGION)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the data from our SparkDataframe before writing it to Neo4j\n",
    "regionCountryDf = sparkDF\\\n",
    "                    .select(['Region','Country'])\\\n",
    "                    .distinct()\\\n",
    "                    .withColumnRenamed(\"Region\", \"regionName\")\\\n",
    "                    .withColumnRenamed(\"Country\", \"countryName\")\n",
    "\n",
    "print(f\"Ingesting {regionCountryDf.count()} relationships from Region to Node called HAS_COUNTRY\")\n",
    "\n",
    "regionCountryDf\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"url\", f\"bolt://{NEO4J_IP_ADDRESS}\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"test\")\\\n",
    "    .option(\"relationship\", \"HAS_COUNTRY\")\\\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\\\n",
    "    .option(\"relationship.source.labels\", \"Region\")\\\n",
    "    .option(\"relationship.source.save.mode\", \"Overwrite\")\\\n",
    "    .option(\"relationship.source.node.keys\", \"regionName:regionName\")\\\n",
    "    .option(\"relationship.target.labels\", \"Country\")\\\n",
    "    .option(\"relationship.target.save.mode\", \"Overwrite\")\\\n",
    "    .option(\"relationship.target.node.keys\", \"countryName:countryName\")\\\n",
    "    .save()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ingesting all the Order nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the data from our SparkDataframe before writing it to Neo4j\n",
    "orderDF = sparkDF\\\n",
    "            .select(['Order ID'])\\\n",
    "            .distinct()\\\n",
    "            .withColumnRenamed(\"Order ID\", \"orderId\")\n",
    "\n",
    "print(f\"Ingesting {orderDF.count()} nodes of type Order\")\n",
    "\n",
    "# When loading nodes using the native format of the Neo4j Connector,\n",
    "# The connector will load all the nodes inside\n",
    "orderDF.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", f\"bolt://{NEO4J_IP_ADDRESS}\")\\\n",
    ".option(\"authentication.basic.username\", \"neo4j\")\\\n",
    ".option(\"authentication.basic.password\", \"test\")\\\n",
    ".option(\"batch.size\", \"10000\")\\\n",
    ".option(\"database\", \"neo4j\")\\\n",
    ".option(\"labels\",\"Order\")\\\n",
    ".option(\"node.keys\",\"orderId\")\\\n",
    ".option(\"script\",\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (t:Order) REQUIRE t.orderId IS UNIQUE;\"\"\")\\\n",
    ".save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the OrderDetail nodes and the relationship they have with their Order (right now we are not using all the data inside the dataset, an OrderDetail objects contains only its total profit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the data from our SparkDataframe before writing it to Neo4j\n",
    "orderDetailDF = sparkDF\\\n",
    "            .select(['Order ID','Order Date','Total Profit',\"Units Sold\",\"Unit Price\",\"Unit Cost\"])\\\n",
    "            .distinct()\\\n",
    "            .withColumnRenamed(\"Order ID\", \"orderId\")\\\n",
    "            .withColumnRenamed(\"Order Date\", \"orderDate\")\\\n",
    "            .withColumnRenamed(\"Units Sold\", \"unitSold\")\\\n",
    "            .withColumnRenamed(\"Unit Price\", \"unitPrice\")\\\n",
    "            .withColumnRenamed(\"Unit Cost\", \"unitCost\")\n",
    "\n",
    "print(f\"Ingesting {orderDetailDF.count()} relationships from Order to OrderDetail nodes called HAS_ORDER_DETAIL\")\n",
    "\n",
    "# Using a query gives us the opportunity to create much more complex injection logics\n",
    "orderDetailDF.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", f\"bolt://{NEO4J_IP_ADDRESS}\")\\\n",
    ".option(\"authentication.basic.username\", \"neo4j\")\\\n",
    ".option(\"authentication.basic.password\", \"test\")\\\n",
    ".option(\"batch.size\", \"10000\")\\\n",
    ".option(\"database\", \"neo4j\")\\\n",
    ".option(\"query\",\"\"\"MATCH (n:Order{orderId: event.orderId})\n",
    "                   WITH event, n, toString(event.orderId) + \"-\" + toString(event.orderDate) as orderDetailId\n",
    "                   MERGE (od:OrderDetail{\n",
    "                                        orderDetailId:orderDetailId,\n",
    "                                        unitSold:event.unitSold,\n",
    "                                        unitPrice:event.unitPrice,\n",
    "                                        unitCost:event.unitCost\n",
    "                                        })\n",
    "                   MERGE (n)-[:HAS_ORDER_DETAIL]->(od)\"\"\")\\\n",
    ".option(\"script\",\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (t:OrderDetail) REQUIRE t.orderDetailId IS UNIQUE;\"\"\")\\\n",
    ".save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load the categories of each OrderDetail, called ItemType (remember to sort them, because we have only 10 categories across almost a million orders, so they're supernodes)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the data from our SparkDataframe before writing it to Neo4j\n",
    "itemTypeOrderDetailDF = sparkDF\\\n",
    "                    .select(['Item Type', 'Order Id', 'Order Date'])\\\n",
    "                    .withColumnRenamed(\"Item Type\", \"itemType\")\\\n",
    "                    .withColumnRenamed(\"Order ID\", \"orderId\")\\\n",
    "                    .withColumnRenamed(\"Order Date\", \"orderDate\")\\\n",
    "                    .sort('itemType')\n",
    "\n",
    "print(f\"Ingesting {itemTypeOrderDetailDF.count()} relationships from OrderDetail to ItemType nodes called HAS_ITEM_TYPE\")\n",
    "\n",
    "# Using a query gives us the opportunity to create much more complex injection logics\n",
    "itemTypeOrderDetailDF.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", f\"bolt://{NEO4J_IP_ADDRESS}\")\\\n",
    ".option(\"authentication.basic.username\", \"neo4j\")\\\n",
    ".option(\"authentication.basic.password\", \"test\")\\\n",
    ".option(\"batch.size\", \"10000\")\\\n",
    ".option(\"database\", \"neo4j\")\\\n",
    ".option(\"query\",\"\"\"WITH event,toString(event.orderId) + \"-\" + toString(event.orderDate) as orderDetailId\n",
    "                   MATCH (od:OrderDetail{orderDetailId: orderDetailId})\n",
    "                   MERGE (it:ItemType{itemType:event.itemType})\n",
    "                   MERGE (od)-[:HAS_ITEM_TYPE]->(it)\"\"\")\\\n",
    ".option(\"script\",\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (it:ItemType) REQUIRE it.itemType IS UNIQUE;\"\"\")\\\n",
    ".save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ingesting all the relationships between an Order and its Country"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the data from our SparkDataframe before writing it to Neo4j\n",
    "countryOrderDf = sparkDF\\\n",
    "                    .select(['Country','Order Id'])\\\n",
    "                    .distinct()\\\n",
    "                    .withColumnRenamed(\"Country\", \"countryName\")\\\n",
    "                    .withColumnRenamed(\"Order Id\", \"orderId\")\n",
    "\n",
    "print(f\"Ingesting {countryOrderDf.count()} relationships from Country to Order nodes called HAS_ORDER\")\n",
    "\n",
    "countryOrderDf\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"org.neo4j.spark.DataSource\")\\\n",
    "    .option(\"url\", f\"bolt://{NEO4J_IP_ADDRESS}\")\\\n",
    "    .option(\"authentication.basic.username\", \"neo4j\")\\\n",
    "    .option(\"authentication.basic.password\", \"test\")\\\n",
    "    .option(\"relationship\", \"HAS_ORDER\")\\\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\\\n",
    "    .option(\"relationship.source.labels\", \"Country\")\\\n",
    "    .option(\"relationship.source.save.mode\", \"Overwrite\")\\\n",
    "    .option(\"relationship.source.node.keys\", \"countryName:countryName\")\\\n",
    "    .option(\"relationship.target.labels\", \"Order\")\\\n",
    "    .option(\"relationship.target.save.mode\", \"Overwrite\")\\\n",
    "    .option(\"relationship.target.node.keys\", \"orderId:orderId\")\\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of all, we should stop the sparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Close the SparkSession\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}